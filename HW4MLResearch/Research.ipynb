{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yQKPJKm19-ll"
   },
   "source": [
    "# Research Paper: Hardware for Machine Learning  \n",
    "- Marcel Castro \n",
    "- Ulises Fonseca \n",
    "\n",
    "# A. Theoretical Research  \n",
    "\n",
    "Machine learning algorithms are characterized by their ability to perform tasks with great complexity. The high capability of these algorithms offers many advantages; however, this also has computational power implications to consider. Currently, there is a constant growth in the creation of new algorithms and their optimizations, which implies opening new possibilities for what computers can execute. As algorithms increase in complexity, it is necessary to have hardware capable of running them. The challenge is to build tools that can support and excel in executing these types of algorithms. Currently, there are different types of hardware used for running machine learning algorithms, including CPUs, GPUs, TPUs, FPGAs, and ASICs. Each of these will be discussed in detail below.\n",
    "\n",
    "## 1. Description and Architecture  \n",
    "#### **CPU**  \n",
    "The CPU (Central Processing Unit) is the standard type of processor found in most general-purpose computers. It handles the operating system, executes programs, and controls the other components of a computer. Modern processors typically have multiple cores, each capable of executing sequential instructions at high speeds. Commercial CPUs usually have between 4 and 24 cores. CPUs have evolved over time to allow parallel execution across different cores; however, the level of parallelism they support can fall behind more specialized hardware, such as GPUs. This performance difference can become a disadvantage when running machine learning algorithms more efficiently.\n",
    "\n",
    "#### **GPU**  \n",
    "The GPU (Graphic Processing Unit) is a type of processor specialized in parallel processing. Initially designed for graphics processing, in recent years it has gained significant interest for machine learning tasks. GPUs specialize in executing a specific type of task, such as graphics processing or running a machine learning algorithm. This contrasts with CPUs, which have a more general purpose since they must coordinate the overall functioning of the computer. Modern GPUs can have hundreds to thousands of cores available for task execution, allowing a high level of parallelism, resulting in faster performance and better efficiency.\n",
    "\n",
    "#### **TPU**  \n",
    "TPU (Tensor Processing Unit) is a type of processor similar to GPUs, but specifically designed to support tensor operations, such as matrix multiplications used in deep learning. Developed by Google in 2016, TPUs are made to work especially with the TensorFlow software. A TPU contains thousands of multiply-accumulate units that allow the hardware to perform matrix multiplications without needing to access registers or shared memory, unlike a GPU which does. They are specifically built for neural network calculations like matrix multiplication.\n",
    "\n",
    "#### **FPGA**  \n",
    "An FPGA (Field Programmable Gate Array) is a type of processor designed to allow the user to configure the hardware to meet specific desired requirements. This chip consists of configurable logic blocks (CLBs) and a set of programmable interconnections that let the user connect and configure blocks to perform functions ranging from simple logic gates to more complex functions. An analogy would be Lego blocks: once arranged in a certain way, they stay like that, but the structure can be disassembled and rearranged differently. FPGAs are very versatile and can be optimized for specific machine learning algorithms.\n",
    "\n",
    "#### **ASIC**  \n",
    "An ASIC (Application Specific Integrated Circuit) is a type of processor designed specifically to perform one function or a series of tasks. An example is chips found in toys, such as a remote-controlled car. They require specialized equipment to manufacture and cannot be reprogrammed; additionally, they cannot execute instructions other than those they were designed for. The goal is to optimize resource consumption and speed. There are semi-custom ASICs (factories have a logic gate template, and connections are made based on client needs) and full-custom ASICs (the entire chip is designed and created completely custom). Examples of ASICs include chips for chargers, switches, or toys. Simple ASICs can be inexpensive, while complex ones can be very costly.\n",
    "\n",
    "## 2. Comparison of Capabilities\n",
    "\n",
    "#### **CPU**  \n",
    "CPUs have great capacity for general computer operation, allowing the execution of multiple tasks simultaneously, which makes them very versatile. Regarding the execution of machine learning algorithms, their capability is somewhat limited; they are generally used for less intensive models that do not require very strong parallelism. They are a good option for machine learning projects of low to medium complexity, and it is worth highlighting that the more complex the algorithm, the more the execution on a CPU becomes compromised. Energy consumption is moderate but can increase with more intensive tasks. Cost varies according to performance, but CPUs can be quite affordable.\n",
    "\n",
    "#### **GPU**  \n",
    "GPUs are currently the most widely used option for running machine learning algorithms. Their parallelism capability allows for very good performance. This type of hardware can process large amounts of data quickly and is specialized in accelerating operations that can be parallelized, such as image processing and matrix operations. GPUs have high parallel processing capabilities, but CPUs perform better in serial processing tasks. GPUs consume a considerable amount of energy, especially for intensive tasks. Costs vary as well, with high-performance models being quite expensive.\n",
    "\n",
    "#### **TPU**  \n",
    "TPUs are designed to work with deep learning operations like matrix multiplication and are optimized to work with TensorFlow. This type of hardware has quite efficient energy consumption relative to the type of processing it performs. Regarding limitations, they are designed for deep learning tasks, so they have limited performance in tasks outside this domain. TPUs are available through Google Cloud and are not accessible for personal use. They are cost-effective for cloud-based processing.\n",
    "\n",
    "#### **FPGA**  \n",
    "FPGAs are flexible due to their configurability to run specific algorithms, which is an advantage for executing algorithms that require specialized processing. They can be quite efficient since they are optimized for specific tasks with lower energy consumption. There are design limitations; specialized hardware design knowledge is required for programming them, and the development and programming costs can be high due to the need for customization.\n",
    "\n",
    "#### **ASIC**  \n",
    "ASICs are very efficient since they are designed specifically for a task, contributing to low energy consumption due to their specificity. They are also very fast precisely because they perform specific tasks. The lack of flexibility is a limitation: once programmed, they cannot be reprogrammed or reused for other tasks. Additionally, the design and development costs can be very high and require large volumes to be cost-effective (the unit cost decreases with large-scale production).\n",
    "\n",
    "## 3. Specialized Architectures for ML\n",
    "\n",
    "#### **Google TPUs**  \n",
    "Google TPUs were specifically designed to work with TensorFlow software. They are specialized to work only with neural networks, focusing on matrix processing—that is, a combination of additions and multiplications. They can handle large models with large batch sizes.\n",
    "\n",
    "#### **Deep Learning Boxes**  \n",
    "Deep Learning Boxes are computers equipped with high-end components for running deep learning algorithms. Generally, the most important component for these executions is the GPU, so a powerful GPU is required to support and excel in algorithm processing. Additionally, other components with good capabilities are important, such as a mid- to high-range CPU, a good amount of RAM (16 to 32 GB), and a solid-state drive for the operating system and other data requiring fast access. It’s also common to have a hard drive with several terabytes of capacity to store large datasets, a motherboard compatible with the CPU and GPU, and a power supply that can handle the power requirements of both. The cost of such equipment is high but justified by very good processing capacity compared to less specialized systems.\n",
    "\n",
    "#### **Embedded AI Systems**  \n",
    "Embedded AI systems are specialized hardware and software components designed to perform AI tasks with a specific purpose, for example, real-time object detection, data analysis for fault prediction, or home automation control based on preferences and sensors. Often these are specialized chips for specific tasks, which help keep energy consumption low and focus on performing a particular task. Specific examples include the growing automotive technology industry. Some cars today can make real-time decisions based on sensor data to help prevent accidents. Another example is agriculture, where irrigation timing can be predicted based on climate and soil data.\n",
    "\n",
    "## 4. Impact of Hardware on Cloud and Edge ML\n",
    "\n",
    "#### **Machine Learning in the Cloud**  \n",
    "Using hardware in the cloud to develop ML applications has great potential. It allows users to access specialized hardware they may not have personally, such as GPUs or TPUs. The cloud also enables scalable processing according to demand, with various services like Google Cloud, AWS, and Google Colab. These services also provide storage for large datasets used in algorithm execution. Cloud usage can optimize costs by leveraging specialized hardware without direct purchase. However, cloud execution can be affected by network quality, making a good internet connection crucial.\n",
    "\n",
    "#### **Machine Learning on Edge Devices**  \n",
    "Edge device processing refers to local data processing instead of cloud-based. This execution requires specialized hardware due to limitations in size, processing power, and energy consumption. Storage capacity and scalability can also be limited, so often heavy processing is done in the cloud while real-time processing happens locally. The advantage of local execution is minimal latency.\n",
    "\n",
    "## 5. Future Trends\n",
    "\n",
    "#### **Expansion and Optimization of TPUs and Specialized Accelerators**  \n",
    "- Due to growth in ML and DL, specialized accelerators will continue evolving, with Google, Nvidia, and others developing even more optimized versions for complex tasks.  \n",
    "- These accelerators will become increasingly efficient in energy consumption and speed, enabling faster processing at lower cloud costs.\n",
    "\n",
    "#### **Proliferation of ASICs for Specific ML Tasks**  \n",
    "- Being designed for specific tasks makes them highly energy-efficient and performant. In the future, more custom ASICs are likely to appear for concrete applications, like computer vision on mobile devices and real-time neural network inference.  \n",
    "- Companies can produce ASICs optimized for certain AI types, making hardware more affordable and AI-capable devices more accessible.\n",
    "\n",
    "#### **Integration of ML Hardware in Edge Devices**  \n",
    "- As demand grows for applications on Edge devices (IoT or smart cars), specialized ML hardware is expected to be increasingly integrated into small, low-power devices.\n",
    "\n",
    "#### **New Hybrid Architectures Combining GPU and CPU**  \n",
    "- This seems to be a future trend, combining the parallel processing power of GPUs with the flexibility of CPUs.  \n",
    "- This may result in hardware capable of handling both parallel and serial processing, improving efficiency in complex applications.\n",
    "\n",
    "#### **Improvements in Energy Efficiency and Sustainability**  \n",
    "- Development of greener materials and more efficient manufacturing processes, focusing on reducing energy consumption and making cloud operations more sustainable in new hardware generations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tpKv9yqgu86j"
   },
   "source": [
    "# B. Practical Research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xy1JML8s-I63"
   },
   "source": [
    "We begin by authorizing access to the drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23407,
     "status": "ok",
     "timestamp": 1731027561335,
     "user": {
      "displayName": "Marcel",
      "userId": "14605507304287611567"
     },
     "user_tz": 360
    },
    "id": "nAfxf_NP-MVl",
    "outputId": "d8c54497-b969-4b3b-9050-75063a80a849"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vgVIVe5F83Lu"
   },
   "source": [
    "# **1. Load of libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3otQMErrAqsV"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rK_QB1fB-f9k"
   },
   "source": [
    "# **2. Dataset Loading and Preprocessing**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W01NZsHE-em_"
   },
   "outputs": [],
   "source": [
    "# Carga del dataset\n",
    "file_path = '/content/drive/My Drive/LabsUlisesMarcel/Investigacion/AMZN_Stock_Updated_V2.csv'\n",
    "dataset = pd.read_csv(file_path)\n",
    "\n",
    "# Se hizo conversión de fecha y eliminación de columnas innecesarias\n",
    "dataset['Date'] = pd.to_datetime(dataset['Date'])\n",
    "dataset.set_index('Date', inplace=True)\n",
    "dataset.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "\n",
    "# Usar solo la columna 'Close' para predicción\n",
    "data = dataset[['Close']]\n",
    "\n",
    "# Escalado de los datos entre 0 y 1\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "# Se crearon secuencias para entrenar el modelo (por ejm, secuencias de 60 días)\n",
    "def create_sequences(data, seq_length=60):\n",
    "    X, y = [], []\n",
    "    for i in range(seq_length, len(data)):\n",
    "        X.append(data[i-seq_length:i, 0])\n",
    "        y.append(data[i, 0])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "seq_length = 60\n",
    "X, y = create_sequences(scaled_data, seq_length)\n",
    "X = np.reshape(X, (X.shape[0], X.shape[1], 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bwvk5eavAEYm"
   },
   "source": [
    "# **3. Neural Network Model Definition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fh9ikX_1AKdJ"
   },
   "outputs": [],
   "source": [
    "def create_lstm_model():\n",
    "    model = Sequential([\n",
    "        LSTM(50, return_sequences=True, input_shape=(X.shape[1], 1)),\n",
    "        LSTM(50, return_sequences=False),\n",
    "        Dense(25),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HQFbzBBaAi7-"
   },
   "source": [
    "This LSTM model has two LSTM layers with 50 units each to capture temporal patterns, followed by two dense (fully connected) layers. The final layer has a single neuron to generate the final price prediction. It is compiled with the `adam` optimizer and the `mean_squared_error` loss function, suitable for regression problems like price prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wHypE85qAtGL"
   },
   "source": [
    "# **4. Model Training and Evaluation on CPU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 193286,
     "status": "ok",
     "timestamp": 1731027768893,
     "user": {
      "displayName": "Marcel",
      "userId": "14605507304287611567"
     },
     "user_tz": 360
    },
    "id": "PzbluQngBX5E",
    "outputId": "47feff6e-7fb3-4859-c3fa-ac8bce448edf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 94ms/step - loss: 0.0079\n",
      "Epoch 2/10\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 87ms/step - loss: 3.1505e-04\n",
      "Epoch 3/10\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 68ms/step - loss: 3.0260e-04\n",
      "Epoch 4/10\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 71ms/step - loss: 2.6314e-04\n",
      "Epoch 5/10\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 70ms/step - loss: 2.3049e-04\n",
      "Epoch 6/10\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 57ms/step - loss: 2.2177e-04\n",
      "Epoch 7/10\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 68ms/step - loss: 2.2374e-04\n",
      "Epoch 8/10\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 62ms/step - loss: 2.2597e-04\n",
      "Epoch 9/10\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 66ms/step - loss: 1.4795e-04\n",
      "Epoch 10/10\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 66ms/step - loss: 1.5958e-04\n",
      "Tiempo de entrenamiento en CPU: 187.28057646751404 segundos\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - loss: 1.7310e-05\n",
      "Pérdida en CPU: 0.00011927514424314722\n"
     ]
    }
   ],
   "source": [
    "# Crea y entrenar el modelo en CPU\n",
    "model_cpu = create_lstm_model()\n",
    "\n",
    "# Mide el tiempo de entrenamiento\n",
    "import time\n",
    "start_time = time.time()\n",
    "history_cpu = model_cpu.fit(X, y, epochs=10, batch_size=32, verbose=1)\n",
    "cpu_training_time = time.time() - start_time\n",
    "print(f\"Tiempo de entrenamiento en CPU: {cpu_training_time} segundos\")\n",
    "\n",
    "# Evaluúa el modelo en CPU\n",
    "loss_cpu = model_cpu.evaluate(X, y)\n",
    "print(f\"Pérdida en CPU: {loss_cpu}\")\n",
    "\n",
    "# Guarda los resultados en diccionario\n",
    "cpu_results = {\n",
    "    'time': cpu_training_time,\n",
    "    'loss': loss_cpu\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kiwxLWehIM1G"
   },
   "source": [
    "# **4. Model Training and Evaluation on GPU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24663,
     "status": "ok",
     "timestamp": 1731027513651,
     "user": {
      "displayName": "Marcel",
      "userId": "14605507304287611567"
     },
     "user_tz": 360
    },
    "id": "JUKi4HSJIVTn",
    "outputId": "3ec87b22-284d-45a6-c2aa-b5403ecc5332"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0083\n",
      "Epoch 2/10\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 3.0485e-04\n",
      "Epoch 3/10\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 2.9586e-04\n",
      "Epoch 4/10\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 2.8924e-04\n",
      "Epoch 5/10\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 1.9411e-04\n",
      "Epoch 6/10\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 2.0074e-04\n",
      "Epoch 7/10\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 1.6053e-04\n",
      "Epoch 8/10\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 1.6779e-04\n",
      "Epoch 9/10\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 1.7246e-04\n",
      "Epoch 10/10\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 1.7009e-04\n",
      "Tiempo de entrenamiento en GPU: 21.408987045288086 segundos\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 1.4790e-05\n",
      "Pérdida en GPU: 9.763328125700355e-05\n"
     ]
    }
   ],
   "source": [
    "# Se crea y entrena el modelo en GPU\n",
    "model_gpu = create_lstm_model()\n",
    "\n",
    "# Medición de tiempo de entrenamiento\n",
    "start_time = time.time()\n",
    "history_gpu = model_gpu.fit(X, y, epochs=10, batch_size=32, verbose=1)\n",
    "gpu_training_time = time.time() - start_time\n",
    "print(f\"Tiempo de entrenamiento en GPU: {gpu_training_time} segundos\")\n",
    "\n",
    "# Se evalúa el modelo\n",
    "loss_gpu = model_gpu.evaluate(X, y)\n",
    "print(f\"Pérdida en GPU: {loss_gpu}\")\n",
    "\n",
    "# Guarda los resultados en un diccionario\n",
    "gpu_results = {\n",
    "    'time': gpu_training_time,\n",
    "    'loss': loss_gpu\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q_-wH3qGNUw7"
   },
   "source": [
    "# **6. Performance and Accuracy Comparison**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sTz6fydhT8fu"
   },
   "source": [
    "### 1. Training Time\n",
    "\n",
    "- **CPU**: Training on CPU took approximately **187.28 seconds**.  \n",
    "- **GPU**: Training on GPU was much faster, taking approximately **21.41 seconds**.\n",
    "\n",
    "The GPU was almost **9 times faster** than the CPU in terms of training time, which is a significant advantage, especially for larger models.\n",
    "\n",
    "### 2. Loss During Training\n",
    "\n",
    "- **CPU**:  \n",
    "  - Loss started at **0.0079** in the first epoch and decreased to **1.5958e-04** in the last epoch.  \n",
    "  - In the final evaluation, the loss was **0.0001193**.\n",
    "\n",
    "- **GPU**:  \n",
    "  - Loss started at **0.0083** in the first epoch and decreased to **1.7009e-04** in the last epoch.  \n",
    "  - At the end, the loss was **9.7633e-05**.\n",
    "\n",
    "Both environments achieved a similar reduction in loss throughout training. The final loss on the GPU is slightly lower than on the CPU, indicating a marginally better model accuracy when using the GPU. However, the difference in loss is small, so both models have comparable accuracy.\n",
    "\n",
    "### 3. Interpretation of Results Analysis\n",
    "\n",
    "- **Time Efficiency**: The GPU proved to be significantly more efficient in terms of training time. This is especially beneficial when training large models, with many data points or a high number of epochs.  \n",
    "  \n",
    "- **Accuracy**: Slight advantage for the GPU. This suggests that the GPU is not only faster but may also provide slightly better accuracy in this case.\n",
    "\n",
    "- **Cost-Benefit**: Considering an environment where GPU usage cost is higher (e.g., in the cloud), this type of analysis shows that the GPU could justify its additional cost in applications requiring fast and frequent training. For tasks that do not require recurrent or large-scale training, a CPU could be sufficient, especially if the budget is limited.\n",
    "\n",
    "In summary, training on a GPU offers a clear speed advantage without sacrificing accuracy, making GPU use ideal for ML tasks involving neural networks or complex models. The CPU, although slower, remains a valid option for occasional training or smaller datasets, especially when time is not a critical factor.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vUQV96LaU7-j"
   },
   "source": [
    "# **7. Cost-Benefit Analysis**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lHrHMECTWxQf"
   },
   "source": [
    "### Summary of Results\n",
    "\n",
    "1. **Training Time**:  \n",
    "   - **CPU**: 187.28 seconds (~3.1 minutes)  \n",
    "   - **GPU**: 21.41 seconds (~0.36 minutes)  \n",
    "\n",
    "   The GPU was almost **9 times faster** than the CPU for the same training.\n",
    "\n",
    "2. **Final Loss**:  \n",
    "   - **CPU**: 0.0001193  \n",
    "   - **GPU**: 0.00009763  \n",
    "\n",
    "   Both devices achieved similar accuracy, with the GPU reaching a slightly lower loss, indicating comparable or marginally better performance in precision.\n",
    "\n",
    "### Analysis Factors\n",
    "\n",
    "1. **Energy Cost**:  \n",
    "   - The GPU consumes more energy compared to the CPU during training, which could increase operational costs. However, the GPU's much shorter usage time reduces the impact of energy consumption in this case.\n",
    "\n",
    "2. **Infrastructure Cost (Cloud)**:  \n",
    "\n",
    "   - When training is done in the cloud, GPU usage is usually more expensive per hour. Suppose the cost is $0.90$ per hour for CPU and $1.50$ per hour for GPU. We estimate the training cost as follows:\n",
    "\n",
    "   - **CPU Cost**:  \n",
    "     $$\n",
    "     \\left(\\frac{187.28 \\, \\text{seconds}}{3600 \\, \\text{seconds per hour}}\\right) \\times 0.90 \\, \\text{USD/hour} = 0.0468 \\, \\text{USD}\n",
    "     $$\n",
    "\n",
    "   - **GPU Cost**:  \n",
    "     $$\n",
    "     \\left(\\frac{21.41 \\, \\text{seconds}}{3600 \\, \\text{seconds per hour}}\\right) \\times 1.50 \\, \\text{USD/hour} = 0.0089 \\, \\text{USD}\n",
    "     $$\n",
    "\n",
    "   - **Conclusion**: Despite the higher hourly cost of GPU, the reduced training time makes the final cost lower on GPU than CPU for this specific task.\n",
    "\n",
    "3. **Infrastructure Cost (Local)**:  \n",
    "   - The initial cost of acquiring a high-performance GPU is significantly higher than that of a CPU. However, this initial investment can be amortized in projects requiring frequent training or large volumes of data, where the GPU offers long-term time and energy savings.  \n",
    "   - In scenarios with occasional training or smaller datasets, the cost of a GPU may not be justified, and a CPU would be sufficient.\n",
    "\n",
    "### Conclusions\n",
    "\n",
    "1. **For frequent training or large-scale projects**:  \n",
    "   - The GPU proves to be a worthwhile investment. It offers considerably faster performance, reducing waiting time for results and optimizing operational cost in terms of cloud resource consumption.  \n",
    "   - Even considering a higher hourly cost, the reduced training time compensates and lowers the total GPU cost compared to CPU.\n",
    "\n",
    "2. **For occasional or smaller-scale training**:  \n",
    "   - The CPU may be sufficient, especially if the budget is limited and training time is not critical.  \n",
    "   - In these cases, savings in infrastructure costs from not using a GPU may not justify specialized hardware use.\n",
    "\n",
    "3. **Scalability and Flexibility**:  \n",
    "   - If the project or application is scalable and data volume is expected to grow over time, investing in a GPU makes sense as it provides greater flexibility and will allow handling higher workloads without significant increases in training costs.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
